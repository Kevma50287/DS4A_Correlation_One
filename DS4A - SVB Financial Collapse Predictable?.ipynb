{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "08350a1e",
   "metadata": {},
   "source": [
    "# SEC - Filtering for Ratios and Expenses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0047c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "JSON_folder_path = './Json'\n",
    "files = os.listdir(JSON_folder_path)\n",
    "file = files[0]\n",
    "\n",
    "\n",
    "with open(JSON_folder_path + \"/\" + file) as jsonFile:\n",
    "    data = json.load(jsonFile)\n",
    "    us_gaap = data['facts']['us-gaap']\n",
    "    keys=[]\n",
    "    formatted_data = {}\n",
    "    formatted_data['entity_name'] = data['entityName']\n",
    "    for key in us_gaap:\n",
    "        keys.append(key.lower())\n",
    "        formatted_data[key] = us_gaap[key]\n",
    "\n",
    "search_key = [\"cash\", \"receivable\", \"payable\", \"asset\", \"liabili\", \"loan\", \"equit\", \"income\", \"expense\", \"interest\", \"deposit\"]\n",
    "print(keys)\n",
    "\n",
    "results={}\n",
    "for key in search_key:\n",
    "    matches = []\n",
    "    for account_name in keys:\n",
    "        if key in account_name:\n",
    "            matches.append(account_name)\n",
    "    results[key]=matches\n",
    "\n",
    "print(results['cash'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ffa13347",
   "metadata": {},
   "source": [
    "# Yahoo Finance Webscraping w/ BeautifulSoup and Selenium\n",
    "\n",
    "How to use:\n",
    "1. Update login() with username and password params\n",
    "2. Update stock_tickers_arr to include tickers of companies to be scraped\n",
    "3. Hit run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fd5f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "\n",
    "# Array of stock tickers to scrape\n",
    "stock_tickers_arr = [\n",
    "    \"SIVBQ\",\"JPM\",\"FRCB\",\n",
    "    \"WFC\",\"BAC\",\"DFS\",\n",
    "    \"MS\",\"PNC\",\"CS\",\n",
    "    \"DB\",\"COF\",\"C\",\n",
    "    \"KEY\",\"BK\",\"GS\",\n",
    "    \"HSBC\",\"MTB\",\"TD\",\n",
    "    \"FHN\",\"OVLY\",\"PACW\",\n",
    "    \"WAL\",\"SBNY\",\"ZION\",\n",
    "    \"CMA\",\"FCNCA\",\"EWBC\",\n",
    "    \"CFG\",\"MCBC\",\"NYCB\",\n",
    "]\n",
    "\n",
    "# Initialize default variables - don't change these\n",
    "path_to = {\n",
    "    'income_statement': \"/financials?p=\",\n",
    "    'balance_sheet':\"/balance-sheet?p=\",\n",
    "    'cashflow_statement':\"/cash-flow?p=\",\n",
    "    'key-statistics':\"/key-statistics?p=\"\n",
    "}\n",
    "stock_ticker = \"JPM\"\n",
    "stock_URL = (\n",
    "    \"https://finance.yahoo.com/quote/\" + stock_ticker + \"/financials?p=\" + stock_ticker\n",
    ")\n",
    "\n",
    "# Functions\n",
    "\n",
    "# Login to account. Yahoo Finance only gives the last 4 reporting periods, need Yahoo Finance Plus for complete hitorical FS data\n",
    "def login(username:str, password:str):\n",
    "    driver.get(stock_URL)\n",
    "\n",
    "    # Login to my account\n",
    "    sign_in_button=driver.find_element(By.ID, 'header-signin-link')\n",
    "    sign_in_button.click()\n",
    "    wait = WebDriverWait(driver, 10)  \n",
    "    element = wait.until(EC.presence_of_element_located((By.ID, 'login-username')))\n",
    "    login_username=driver.find_element(By.ID, 'login-username')\n",
    "\n",
    "    # Enter username\n",
    "    login_username.send_keys(username)\n",
    "    wait = WebDriverWait(driver, 10)  \n",
    "    element = wait.until(EC.presence_of_element_located((By.ID, 'login-signin')))\n",
    "    login_sign_in_button=driver.find_element(By.ID, 'login-signin')\n",
    "    login_sign_in_button.click()\n",
    "\n",
    "    # Maximum wait time of 10 seconds, wait for element to load\n",
    "    wait = WebDriverWait(driver, 10)  \n",
    "    element = wait.until(EC.presence_of_element_located((By.ID, 'login-passwd')))\n",
    "    login_passwd = driver.find_element(By.ID, 'login-passwd')\n",
    "\n",
    "    # Enter password\n",
    "    login_passwd.send_keys(password)\n",
    "    login_sign_in_button=driver.find_element(By.ID, 'login-signin')\n",
    "    login_sign_in_button.click()\n",
    "\n",
    "def get_URL_to(ticker: str, financial_statement_name: str):\n",
    "    stock_ticker = ticker\n",
    "    path = path_to[financial_statement_name]\n",
    "    URL = (\n",
    "        \"https://finance.yahoo.com/quote/\" + stock_ticker + path + stock_ticker\n",
    "    )\n",
    "    return URL\n",
    "\n",
    "#Only go to URL if it is different than the current URL\n",
    "def go_to_URL(URL):\n",
    "    current_url = driver.current_url\n",
    "    driver.get(URL)\n",
    "\n",
    "def expand_all_get_html():\n",
    "    time.sleep(2)\n",
    "    # Wait for expand button to load before clicking\n",
    "    wait = WebDriverWait(driver, 10)  \n",
    "    element = wait.until(EC.presence_of_element_located((By.CLASS_NAME, 'expandPf')))\n",
    "    expand_button = driver.find_element(By.CLASS_NAME, 'expandPf')\n",
    "    expand_button.click()\n",
    "\n",
    "    # Let page load new html before extracting\n",
    "    time.sleep(2)\n",
    "\n",
    "    # Download HTML file\n",
    "    html_source = driver.page_source\n",
    "\n",
    "    return html_source\n",
    "\n",
    "def create_directory_if_not_exists(ticker):\n",
    "    directory = f'./CSV/{ticker}'\n",
    "    # Create the directory if it doesn't exist\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "def scrape_and_save_to_csv(html, ticker:str, financial_statement_name:str, period:str):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # Find all table headers (Dates)\n",
    "    table_headers = soup.find(\"div\", {\"class\": \"D(tbhg)\"})\n",
    "    table_headers_row = table_headers.find_all(\"div\", {\"class\": \"D(ib)\"})\n",
    "    table_headers_names = []\n",
    "\n",
    "    for div in table_headers_row:\n",
    "        span = div.span\n",
    "        name = span.get_text()\n",
    "        table_headers_names.append(name)\n",
    "\n",
    "    table_column_length = len(table_headers_names)\n",
    "\n",
    "    # Find all table rows (Amounts)\n",
    "    table_rows = soup.find(\"div\", {\"class\": \"D(tbrg)\"})\n",
    "    table_rows_data = table_rows.find_all(\"div\", {\"class\": \"D(tbr)\"})\n",
    "    table_rows_data_arrays = []\n",
    "\n",
    "    for table_row in table_rows_data:\n",
    "        row_header_text = table_row.find(\"div\", {'class':\"Ta(start)\"}).find(\"span\").get_text()\n",
    "        row_data_array = table_row.find_all(\"div\", {'class':\"Ta(c)\"})\n",
    "        row_data_text_array = [row_header_text]\n",
    "\n",
    "        for div in row_data_array:\n",
    "            span = div.find(\"span\") if div.find(\"span\") != None else div\n",
    "            text = span.get_text()\n",
    "            row_data_text_array.append(text)\n",
    "        table_rows_data_arrays.append(row_data_text_array)\n",
    "\n",
    "\n",
    "    data_frame_object = {table_headers_names[0]:table_headers_names[1:]}\n",
    "\n",
    "    # First element in row array contains the row header\n",
    "    # Map it to an object and then convert into a dataframe\n",
    "    for data in table_rows_data_arrays:\n",
    "        data_frame_object[data[0]] = data[1:]\n",
    "\n",
    "    df = pd.DataFrame(data_frame_object)\n",
    "    # Save CSV\n",
    "    df.to_csv(f'./CSV/{ticker}/{financial_statement_name}_{period}.csv')\n",
    "\n",
    "def save_one_financial_statement(ticker:str, financial_statement_name:str, period:str):\n",
    "    create_directory_if_not_exists(ticker)\n",
    "    if period == 'quarterly':\n",
    "        URL = get_URL_to(ticker, financial_statement_name)\n",
    "        go_to_URL(URL)\n",
    "        wait = WebDriverWait(driver, 10)  \n",
    "        element = wait.until(EC.presence_of_element_located((By.XPATH, '//button[div[span[text()=\"Quarterly\"]]]')))\n",
    "        quarterly_button = driver.find_element(By.XPATH, '//button[div[span[text()=\"Quarterly\"]]]')\n",
    "        quarterly_button.click()\n",
    "        html_source = expand_all_get_html()\n",
    "        scrape_and_save_to_csv(html_source, ticker, financial_statement_name, period)\n",
    "    else:\n",
    "        URL = get_URL_to(ticker, financial_statement_name)\n",
    "        go_to_URL(URL)\n",
    "        html_source = expand_all_get_html()\n",
    "        scrape_and_save_to_csv(html_source, ticker, financial_statement_name, period)\n",
    "\n",
    "def save_all_financial_statements(ticker:str):\n",
    "    save_one_financial_statement(ticker, 'income_statement', 'annual')\n",
    "    save_one_financial_statement(ticker, 'income_statement', 'quarterly')\n",
    "    save_one_financial_statement(ticker, 'balance_sheet', 'annual')\n",
    "    save_one_financial_statement(ticker, 'balance_sheet', 'quarterly')\n",
    "    save_one_financial_statement(ticker, 'cashflow_statement', 'annual')\n",
    "    save_one_financial_statement(ticker, 'cashflow_statement', 'quarterly')\n",
    "\n",
    "def save_market_cap(ticker:str, period:str):\n",
    "    URL = get_URL_to(ticker, 'key-statistics')\n",
    "    go_to_URL(URL)\n",
    "    wait = WebDriverWait(driver, 10)  \n",
    "    element = wait.until(EC.presence_of_element_located((By.XPATH, f'//button[div[span[text()=\"{period}\"]]]')))\n",
    "    button = driver.find_element(By.XPATH, f'//button[div[span[text()=\"{period}\"]]]')\n",
    "    button.click()\n",
    "    html_source = driver.page_source\n",
    "    soup = BeautifulSoup(html_source, \"html.parser\")\n",
    "\n",
    "    # Find all table headers (Dates)\n",
    "    table_headers = soup.find(\"thead\")\n",
    "    table_headers_row = table_headers.find_all(\"th\")\n",
    "    table_headers_names = []\n",
    "    for th in table_headers_row:\n",
    "        print(th)\n",
    "        span = th.find('span')\n",
    "        if span:\n",
    "            text = span.get_text()\n",
    "        else:\n",
    "            text = \"\"\n",
    "        table_headers_names.append(text)\n",
    "\n",
    "    table_column_length = len(table_headers_names)\n",
    "\n",
    "    # Find all table rows (Amounts)\n",
    "    table_rows = soup.find(\"tbody\")\n",
    "    table_rows_data = table_rows.find_all(\"tr\")\n",
    "    table_rows_data_arrays = []\n",
    "    for tr in table_rows_data:\n",
    "        row_data_array = []\n",
    "        for i, td in enumerate(tr):\n",
    "            if i == 0:\n",
    "                row_data_array.append(td.span.get_text())\n",
    "            else:\n",
    "                row_data_array.append(td.get_text())\n",
    "        table_rows_data_arrays.append(row_data_array)\n",
    "    \n",
    "    data_frame_object = {table_headers_names[0]:table_headers_names[1:]}\n",
    "\n",
    "    # First element in row array contains the row header\n",
    "    # Map it to an object and then convert into a dataframe\n",
    "    for data in table_rows_data_arrays:\n",
    "        data_frame_object[data[0]] = data[1:]\n",
    "\n",
    "    df = pd.DataFrame(data_frame_object)\n",
    "    # Save CSV\n",
    "    df.to_csv(f'./CSV/{ticker}/market_stats.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3eebd9",
   "metadata": {},
   "source": [
    "### Compilation of all appropriate CSV files\n",
    "\n",
    "**More info to be added soon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a42081",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compilation of all appropriate csv files\n",
    "main_path = os.getcwd() + \"/CSV\"\n",
    "\n",
    "# Initializing dicts\n",
    "balance_sheet_df_collection = {}\n",
    "cashflow_statement_df_collection = {}\n",
    "income_statement_df_collection = {}\n",
    "market_stats_df_collection = {}\n",
    "\n",
    "collections = {\n",
    "    \"balance_sheet_quarterly.csv\": balance_sheet_df_collection,\n",
    "    \"cashflow_statement_quarterly.csv\": cashflow_statement_df_collection,\n",
    "    \"income_statement_quarterly.csv\": income_statement_df_collection,\n",
    "    \"market_stats.csv\": market_stats_df_collection\n",
    "}\n",
    "\n",
    "def process_csv_file(current_path, folder):\n",
    "    copy_df = pd.read_csv(current_path)\n",
    "    copy_df[\"Unnamed: 0\"] = (copy_df[\"Unnamed: 0\"] >= 0).astype(int).replace(1, folder)\n",
    "    column_names = copy_df.columns\n",
    "    return pd.DataFrame(copy_df, columns=column_names)\n",
    "\n",
    "for folder in os.listdir(main_path):\n",
    "    for csv_file in os.listdir(os.path.join(main_path, folder)):\n",
    "        current_path = os.path.join(main_path, folder, csv_file)\n",
    "        \n",
    "        if csv_file in collections:\n",
    "            collections[csv_file][folder] = process_csv_file(current_path, folder)\n",
    "\n",
    "\n",
    "balance_sheet_df = pd.concat(balance_sheet_df_collection)\n",
    "cashflow_statement_df = pd.concat(cashflow_statement_df_collection)\n",
    "income_statement_df = pd.concat(income_statement_df_collection)\n",
    "market_stats_df = pd.concat(market_stats_df_collection)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e1ac92",
   "metadata": {},
   "source": [
    "### Clean up of compiled dataframes\n",
    "\n",
    " - balance_sheet_df\n",
    " - cashflow_statement_df\n",
    " - income_statement_df\n",
    " - market_stats_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b0ad3d",
   "metadata": {},
   "source": [
    "### Balance Sheet Data Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97677c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "balance_sheet_df.rename(columns = {\n",
    "    \"Unnamed: 0\":\"Ticker\",\n",
    "    \"Breakdown\":\"Date\",\n",
    "    \"SECURITIES_AND_INVESTMENTS\":\"Securities And Investments\",\n",
    "    \"Accounts receivables\":\"Accounts Receivables\"\n",
    "    }, inplace = True)\n",
    "# print(balance_sheet_df)\n",
    "balance_sheet_df[\"Date\"] = pd.to_datetime(balance_sheet_df[\"Date\"])\n",
    "balance_sheet_df.index.set_names(['Ticker Symbol', 'Index'], inplace = True) # Set index names \n",
    "# print(balance_sheet_df)\n",
    "# Clean column types\n",
    "cols=[i for i in balance_sheet_df.columns if i not in [\"Ticker\",\"Date\"]]\n",
    "for col in cols:\n",
    "    balance_sheet_df[col] = balance_sheet_df[col].astype(str).str.replace(',','')\n",
    "    balance_sheet_df[col].replace('-', np.nan, inplace = True) #double check no negative values, find way to replace \"-\" for null values NaN without replacing negative values\n",
    "    balance_sheet_df[col]= balance_sheet_df[col].astype(float)\n",
    "\n",
    "balance_sheet_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047f426a",
   "metadata": {},
   "source": [
    "### Cashflow Statement Data Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d8e900",
   "metadata": {},
   "outputs": [],
   "source": [
    "cashflow_statement_df.rename(columns = {\"Unnamed: 0\":\"Ticker\", \"Breakdown\":\"Date\"}, inplace = True)\n",
    "cashflow_statement_df.index.set_names(['Ticker Symbol', 'Index'], inplace = True) # Set index names \n",
    "cashflow_statement_df.drop(0, level=1, axis=0, inplace=True) # Erase all ttm values as not necessary (trailing twelve months data not necessary)\n",
    "cashflow_statement_df[\"Date\"] = pd.to_datetime(cashflow_statement_df[\"Date\"]) # Updating column type to datetime\n",
    "\n",
    "# Clean column types\n",
    "cols=[i for i in cashflow_statement_df.columns if i not in [\"Ticker\",\"Date\"]]\n",
    "for col in cols:\n",
    "    cashflow_statement_df[col] = cashflow_statement_df[col].astype(str).str.replace(',','')\n",
    "    cashflow_statement_df[col].replace('-', np.nan, inplace = True) #double check no negative values, find way to replace \"-\" for null values NaN without replacing negative values but it works?\n",
    "    cashflow_statement_df[col]= cashflow_statement_df[col].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529dabd3",
   "metadata": {},
   "source": [
    "### Income Statement Data Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7377168",
   "metadata": {},
   "outputs": [],
   "source": [
    "income_statement_df.rename(columns = {\"Unnamed: 0\":\"Ticker\", \"Breakdown\":\"Date\"}, inplace = True)\n",
    "income_statement_df.index.set_names(['Ticker Symbol', 'Index'], inplace = True) # Set index names \n",
    "income_statement_df.drop(0, level=1, axis=0, inplace=True) # Erase all ttm values as not necessary (trailing twelve months data not necessary)\n",
    "income_statement_df[\"Date\"] = pd.to_datetime(income_statement_df[\"Date\"]) # Updating column type to datetime\n",
    "\n",
    "# Clean column types\n",
    "cols=[i for i in income_statement_df.columns if i not in [\"Ticker\",\"Date\"]]\n",
    "for col in cols:\n",
    "    income_statement_df[col] = income_statement_df[col].astype(str).str.replace(',','')\n",
    "    income_statement_df[col].replace('-', np.nan, inplace = True) #double check no negative values, find way to replace \"-\" for null values NaN without replacing negative values but it works?\n",
    "    income_statement_df[col]= income_statement_df[col].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4fefe8",
   "metadata": {},
   "source": [
    "### Market Stats Data Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595fcefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "market_stats_df.rename(columns = {\n",
    "    \"Unnamed: 0\":\"Ticker\", \n",
    "    \"Breakdown\":\"Date\",\n",
    "    \"Market Cap (intraday)\":\"Market Cap Intraday\",\n",
    "    \"Trailing P/E\":\"Trailing PE\",\n",
    "    \"Forward P/E\":\"Forward PE\",\n",
    "    \"PEG Ratio (5 yr expected)\":\"PEG Ratio 5 Yr Expected\",\n",
    "    \"Price/Sales\":\"Price Over Sales\",\n",
    "    \"Price/Book\":\"Price Over Book\",\n",
    "    \"Enterprise Value/Revenue\":\"Enterprise Value Over Revenue\",\n",
    "    \"Enterprise Value/EBITDA\":\"Enterprise Value Over EBITDA\"\n",
    "    }, inplace = True)\n",
    "market_stats_df.index.set_names(['Ticker Symbol', 'Index'], inplace = True) # Set index names \n",
    "market_stats_df.drop(0, level=1, axis=0, inplace=True) # Erase all \"as of date\" rows (1st row for every ticker - unnecessary data)\n",
    "market_stats_df[\"Date\"] = pd.to_datetime(market_stats_df[\"Date\"]) # Updating column type to datetime\n",
    "\n",
    "# Clean column types (all numerical columns except market cap are floats)\n",
    "cols=[i for i in market_stats_df.columns if i not in [\"Ticker\",\"Date\"]]\n",
    "for col in cols:\n",
    "    market_stats_df[col].replace('-', np.nan, inplace = True) #double check no negative values, find way to replace \"-\" for null values NaN without replacing negative values but it works?\n",
    "\n",
    "# replace \".\" with \"\" in only market cap and replace M with 4 zeros, replace B with 7 zeros\n",
    "market_stats_df[\"Market Cap (intraday)\"] = market_stats_df[\"Market Cap (intraday)\"].astype(str).str.replace(\"B\", \"0000000\")\n",
    "market_stats_df[\"Market Cap (intraday)\"] = market_stats_df[\"Market Cap (intraday)\"].astype(str).str.replace(\"M\", \"0000\")\n",
    "market_stats_df[\"Market Cap (intraday)\"] = market_stats_df[\"Market Cap (intraday)\"].astype(str).str.replace(\".\", \"\")\n",
    "market_stats_df[\"Market Cap (intraday)\"] = market_stats_df[\"Market Cap (intraday)\"].astype(str).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e21d72",
   "metadata": {},
   "source": [
    "### Filter dates for all files to Jan 2017 - March 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755d89b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store the DataFrames\n",
    "dataframes = {\n",
    "    'balance_sheet_df': balance_sheet_df,\n",
    "    'cashflow_statement_df': cashflow_statement_df,\n",
    "    'income_statement_df': income_statement_df,\n",
    "    'market_stats_df': market_stats_df\n",
    "}\n",
    "\n",
    "# Define the cutoff date\n",
    "cutoff_earliest_date = pd.to_datetime('2016-12-31')\n",
    "cutoff_latest_date = pd.to_datetime('2022-04-01')\n",
    "\n",
    "# Iterate over the dictionary items and filter the DataFrames\n",
    "for df_name, df in dataframes.items():\n",
    "    mask = (df['Date'] >= cutoff_earliest_date) & (df['Date'] <= cutoff_latest_date)\n",
    "    dataframes[df_name] = df[mask]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99d7cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the updated DataFrames\n",
    "balance_sheet_df = dataframes['balance_sheet_df'].reset_index().drop(columns=[\"Index\", \"Ticker\"])\n",
    "cashflow_statement_df = dataframes['cashflow_statement_df'].reset_index().drop(columns=[\"Index\", \"Ticker\"])\n",
    "income_statement_df = dataframes['income_statement_df'].reset_index().drop(columns=[\"Index\", \"Ticker\"])\n",
    "market_stats_df = dataframes['market_stats_df'].reset_index().drop(columns=[\"Index\", \"Ticker\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce85433d",
   "metadata": {},
   "source": [
    "### df to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b39ea10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves compiled dataframes as csv files under compiled_CSV\n",
    "os.makedirs('compiled_CSV', exist_ok=True)  \n",
    "balance_sheet_df.to_csv('compiled_CSV/balance_sheet.csv') \n",
    "cashflow_statement_df.to_csv('compiled_CSV/cashflow_statement.csv') \n",
    "income_statement_df.to_csv('compiled_CSV/income_statement.csv') \n",
    "market_stats_df.to_csv('compiled_CSV/market_stats.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2a2761",
   "metadata": {},
   "source": [
    "### Loading objects/csv files to AWS S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ee6ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "access_key = os.environ.get(\"AWS_ACCESS_KEY_ID\")\n",
    "secret_key = os.environ.get(\"AWS_SECRET_ACCESS_KEY\")\n",
    "\n",
    "def get_file_paths(folder_path):\n",
    "    file_paths = []\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.csv'):  # Filter CSV files\n",
    "                file_path = os.path.join(root, file)\n",
    "                file_paths.append(file_path)\n",
    "    return file_paths\n",
    "\n",
    "def upload_files_to_s3(file_paths, bucket_name):\n",
    "    s3 = boto3.client('s3', aws_access_key_id=access_key, aws_secret_access_key=secret_key)\n",
    "    for file_path in file_paths:\n",
    "        file_name = file_path.split('/')[-1]  # Extract the file name from the path\n",
    "        s3.upload_file(file_path, bucket_name, file_name)\n",
    "        print(f\"Uploaded {file_name} to {bucket_name}\")\n",
    "\n",
    "# Folder path containing the CSV files\n",
    "folder_path = os.getcwd() + \"/compiled_CSV\"\n",
    "\n",
    "# Call the function to get file paths\n",
    "file_paths = get_file_paths(folder_path)\n",
    "\n",
    "# Name of the S3 bucket\n",
    "bucket_name = 'ds4a-c1-team22'\n",
    "\n",
    "# Call the function to upload files\n",
    "upload_files_to_s3(file_paths, bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cf8120",
   "metadata": {},
   "source": [
    "### Company Info CSV cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7f1985",
   "metadata": {},
   "outputs": [],
   "source": [
    "market_stats_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276230a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "balance_sheet_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfe6d09",
   "metadata": {},
   "source": [
    "### Financial Ratio Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f9b36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil.relativedelta import relativedelta\n",
    "import datetime\n",
    "\n",
    "financial_ratios = market_stats_df.drop(columns = [\n",
    "    'Enterprise Value','Trailing PE', 'Forward PE', 'PEG Ratio 5 Yr Expected','Price Over Sales', \n",
    "    'Price Over Book', 'Enterprise Value Over Revenue', 'Enterprise Value Over EBITDA'])\n",
    "\n",
    "financial_ratios[\"ROE\"] = (income_statement_df[\"Net Income Common Stockholders\"] / balance_sheet_df[\"Common Stock Equity\"]) * 100\n",
    "financial_ratios[\"ROA\"] = (income_statement_df[\"Net Income Common Stockholders\"] / balance_sheet_df[\"Total Assets\"]) * 100\n",
    "\n",
    "\n",
    "d1 = income_statement_df[[\"Date\", \"Ticker\",\"Net Interest Income\"]]\n",
    "d2 = balance_sheet_df[[\"Date\", \"Ticker\",\"Total Assets\"]]\n",
    "\n",
    "\n",
    "merge_NI_and_TA = pd.merge(d1, d2, on=['Date', 'Ticker'])\n",
    "merge_NI_and_TA[merge_NI_and_TA[\"Date\"] == merge_NI_and_TA[\"Date\"] + pd.DateOffset(years=-1)]\n",
    "\n",
    "# for index, row in merge_NI_and_TA.iterrows():\n",
    "#     curr_year_total_assets = row[\"Total Assets\"]\n",
    "#     prior_year = pd.Timestamp(row[\"Date\"].date() - relativedelta(years=1))\n",
    "#     prior_year_total_assets = merge_NI_and_TA[merge_NI_and_TA[\"Date\"] == prior_year][\"Total Assets\"]\n",
    "#     # print(row[\"Net Interest Income\"], curr_year_total_assets, prior_year_total_assets)\n",
    "#     NI = row[\"Net Interest Income\"]\n",
    "#     val = NI / ((curr_year_total_assets + prior_year_total_assets) / 2)\n",
    "#     print(merge_NI_and_TA.loc(merge_NI_and_TA[\"Average Total Assets\"]))\n",
    "#     print(merge_NI_and_TA.loc[ (merge_NI_and_TA[\"Date\"] == row[\"Date\"]) & (merge_NI_and_TA[\"Ticker\"] == row[\"Ticker\"]) ][\"Average Total Assets\"])\n",
    "\n",
    "\n",
    "# financial_ratios[\"NIM\"] = (income_statement_df[\"Net Interest Income\"] / ((balance_sheet_df)/ 2)#missing average invested assets\n",
    "\n",
    "#financial_ratios[\"NPL\"] =\n",
    "\n",
    "# financial_ratios"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1d427398",
   "metadata": {},
   "source": [
    "### Apache Airflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d246dd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Script execution\n",
    "def extract_script_execution():\n",
    "    stock_tickers_arr = [\n",
    "        # Add Stock Symbols here\n",
    "    ]\n",
    "\n",
    "    # Install and set ChromeDriver\n",
    "    driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))\n",
    "\n",
    "    # Login here\n",
    "    login(\"kevinator112233@yahoo.com\", \"\")\n",
    "\n",
    "    for stock_ticker in stock_tickers_arr:\n",
    "        save_all_financial_statements(stock_ticker)\n",
    "        save_market_cap(stock_ticker)\n",
    "    \n",
    "    driver.quit()\n",
    "\n",
    "def transform():\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c2afbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "\n",
    "import pendulum\n",
    "\n",
    "from airflow.decorators import dag, task\n",
    "@dag(\n",
    "    schedule=None,\n",
    "    start_date=pendulum.datetime(2023, 9, 30, tz=\"UTC\"),\n",
    "    catchup=False,\n",
    "    tags=[\"example\"],\n",
    ")\n",
    "def tutorial_taskflow_api():\n",
    "    \"\"\"\n",
    "    ### TaskFlow API Tutorial Documentation\n",
    "    This is a simple data pipeline example which demonstrates the use of\n",
    "    the TaskFlow API using three simple tasks for Extract, Transform, and Load.\n",
    "    Documentation that goes along with the Airflow TaskFlow API tutorial is\n",
    "    located\n",
    "    [here](https://airflow.apache.org/docs/apache-airflow/stable/tutorial_taskflow_api.html)\n",
    "    \"\"\"\n",
    "    @task()\n",
    "    def extract():\n",
    "        \"\"\"\n",
    "        #### Extract task\n",
    "        \"\"\"\n",
    "        # TODO: Remove this return\n",
    "        return True\n",
    "        stock_tickers_arr = [\n",
    "        # Add Stock Symbols here\n",
    "        ]\n",
    "\n",
    "        # Install and set ChromeDriver\n",
    "        driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))\n",
    "\n",
    "        # Login here\n",
    "        login(\"kevinator112233@yahoo.com\", \"\")\n",
    "\n",
    "        for stock_ticker in stock_tickers_arr:\n",
    "            save_all_financial_statements(stock_ticker)\n",
    "            save_market_cap(stock_ticker)\n",
    "\n",
    "        driver.quit()\n",
    "    @task(multiple_outputs=True)\n",
    "    def compile():\n",
    "        \"\"\"\n",
    "        #### Compile task\n",
    "        A simple Transform task which takes in the collection of order data and\n",
    "        computes the total order value.\n",
    "        \"\"\"\n",
    "\n",
    "        #compilation of all appropriate csv files\n",
    "        main_path = os.getcwd() + \"/CSV\"\n",
    "\n",
    "        # Initializing dicts\n",
    "        balance_sheet_df_collection = {}\n",
    "        cashflow_statement_df_collection = {}\n",
    "        income_statement_df_collection = {}\n",
    "        market_stats_df_collection = {}\n",
    "\n",
    "        collections = {\n",
    "            \"balance_sheet_quarterly.csv\": balance_sheet_df_collection,\n",
    "            \"cashflow_statement_quarterly.csv\": cashflow_statement_df_collection,\n",
    "            \"income_statement_quarterly.csv\": income_statement_df_collection,\n",
    "            \"market_stats.csv\": market_stats_df_collection\n",
    "        }\n",
    "\n",
    "        def process_csv_file(current_path, folder):\n",
    "            copy_df = pd.read_csv(current_path)\n",
    "            copy_df[\"Unnamed: 0\"] = (copy_df[\"Unnamed: 0\"] >= 0).astype(int).replace(1, folder)\n",
    "            column_names = copy_df.columns\n",
    "            return pd.DataFrame(copy_df, columns=column_names)\n",
    "\n",
    "        for folder in os.listdir(main_path):\n",
    "            for csv_file in os.listdir(os.path.join(main_path, folder)):\n",
    "                current_path = os.path.join(main_path, folder, csv_file)\n",
    "                if csv_file in collections:\n",
    "                    collections[csv_file][folder] = process_csv_file(current_path, folder)\n",
    "\n",
    "        balance_sheet_df = pd.concat(balance_sheet_df_collection)\n",
    "        cashflow_statement_df = pd.concat(cashflow_statement_df_collection)\n",
    "        income_statement_df = pd.concat(income_statement_df_collection)\n",
    "        market_stats_df = pd.concat(market_stats_df_collection)\n",
    "        return {\n",
    "            balance_sheet_df:balance_sheet_df,\n",
    "            cashflow_statement_df:cashflow_statement_df,\n",
    "            income_statement_df:income_statement_df,\n",
    "            market_stats_df:market_stats_df\n",
    "        }\n",
    "    @task()\n",
    "    def transform(dataframes:object):\n",
    "\n",
    "        # BALANCE SHEET\n",
    "\n",
    "        balance_sheet_df.rename(columns = {\"Unnamed: 0\":\"Ticker\", \"Breakdown\":\"Date\"}, inplace = True)\n",
    "        balance_sheet_df[\"Date\"] = pd.to_datetime(balance_sheet_df[\"Date\"])\n",
    "        balance_sheet_df.index.set_names(['Ticker Symbol', 'Index'], inplace = True) # Set index names \n",
    "\n",
    "        # Clean column types\n",
    "        cols=[i for i in balance_sheet_df.columns if i not in [\"Ticker\",\"Date\"]]\n",
    "        for col in cols:\n",
    "            balance_sheet_df[col] = balance_sheet_df[col].astype(str).str.replace(',','')\n",
    "            balance_sheet_df[col].replace('-', np.nan, inplace = True) #double check no negative values, find way to replace \"-\" for null values NaN without replacing negative values\n",
    "            balance_sheet_df[col]= balance_sheet_df[col].astype(float)\n",
    "\n",
    "        # CASH FLOWS\n",
    "        cashflow_statement_df.rename(columns = {\"Unnamed: 0\":\"Ticker\", \"Breakdown\":\"Date\"}, inplace = True)\n",
    "        cashflow_statement_df.index.set_names(['Ticker Symbol', 'Index'], inplace = True) # Set index names \n",
    "        cashflow_statement_df.drop(0, level=1, axis=0, inplace=True) # Erase all ttm values as not necessary (trailing twelve months data not necessary)\n",
    "        cashflow_statement_df[\"Date\"] = pd.to_datetime(cashflow_statement_df[\"Date\"]) # Updating column type to datetime\n",
    "        \n",
    "        # Clean column types\n",
    "        cols=[i for i in cashflow_statement_df.columns if i not in [\"Ticker\",\"Date\"]]\n",
    "        for col in cols:\n",
    "            cashflow_statement_df[col] = cashflow_statement_df[col].astype(str).str.replace(',','')\n",
    "            cashflow_statement_df[col].replace('-', np.nan, inplace = True) #double check no negative values, find way to replace \"-\" for null values NaN without replacing negative values but it works?\n",
    "            cashflow_statement_df[col]= cashflow_statement_df[col].astype(float)\n",
    "\n",
    "        # INCOME STATEMENT\n",
    "        income_statement_df.rename(columns = {\"Unnamed: 0\":\"Ticker\", \"Breakdown\":\"Date\"}, inplace = True)\n",
    "        income_statement_df.index.set_names(['Ticker Symbol', 'Index'], inplace = True) # Set index names \n",
    "        income_statement_df.drop(0, level=1, axis=0, inplace=True) # Erase all ttm values as not necessary (trailing twelve months data not necessary)\n",
    "        income_statement_df[\"Date\"] = pd.to_datetime(income_statement_df[\"Date\"]) # Updating column type to datetime\n",
    "\n",
    "        # Clean column types\n",
    "        cols=[i for i in income_statement_df.columns if i not in [\"Ticker\",\"Date\"]]\n",
    "        for col in cols:\n",
    "            income_statement_df[col] = income_statement_df[col].astype(str).str.replace(',','')\n",
    "            income_statement_df[col].replace('-', np.nan, inplace = True) #double check no negative values, find way to replace \"-\" for null values NaN without replacing negative values but it works?\n",
    "            income_statement_df[col]= income_statement_df[col].astype(float)\n",
    "\n",
    "        # MARKET STATS\n",
    "        market_stats_df.rename(columns = {\"Unnamed: 0\":\"Ticker\", \"Breakdown\":\"Date\"}, inplace = True)\n",
    "        market_stats_df.index.set_names(['Ticker Symbol', 'Index'], inplace = True) # Set index names \n",
    "        market_stats_df.drop(0, level=1, axis=0, inplace=True) # Erase all \"as of date\" rows (1st row for every ticker - unnecessary data)\n",
    "        market_stats_df[\"Date\"] = pd.to_datetime(market_stats_df[\"Date\"]) # Updating column type to datetime\n",
    "        \n",
    "        # Clean column types (all numerical columns except market cap are floats)\n",
    "        cols=[i for i in market_stats_df.columns if i not in [\"Ticker\",\"Date\"]]\n",
    "        for col in cols:\n",
    "            market_stats_df[col].replace('-', np.nan, inplace = True) #double check no negative values, find way to replace \"-\" for null values NaN without replacing negative values but it works?\n",
    "        \n",
    "        # replace \".\" with \"\" in only market cap and replace M with 4 zeros, replace B with 7 zeros\n",
    "        market_stats_df[\"Market Cap (intraday)\"] = market_stats_df[\"Market Cap (intraday)\"].astype(str).str.replace(\"B\", \"0000000\")\n",
    "        market_stats_df[\"Market Cap (intraday)\"] = market_stats_df[\"Market Cap (intraday)\"].astype(str).str.replace(\"M\", \"0000\")\n",
    "        market_stats_df[\"Market Cap (intraday)\"] = market_stats_df[\"Market Cap (intraday)\"].astype(str).str.replace(\".\", \"\")\n",
    "        market_stats_df[\"Market Cap (intraday)\"] = market_stats_df[\"Market Cap (intraday)\"].astype(str).astype(int)\n",
    "\n",
    "        dataframes = {\n",
    "            'balance_sheet_df': balance_sheet_df,\n",
    "            'cashflow_statement_df': cashflow_statement_df,\n",
    "            'income_statement_df': income_statement_df,\n",
    "            'market_stats_df': market_stats_df\n",
    "        }\n",
    "        \n",
    "        # Define the cutoff date\n",
    "        cutoff_earliest_date = pd.to_datetime('2016-12-31')\n",
    "        cutoff_latest_date = pd.to_datetime('2022-04-01')\n",
    "        \n",
    "        # Iterate over the dictionary items and filter the DataFrames\n",
    "        for df_name, df in dataframes.items():\n",
    "            mask = (df['Date'] >= cutoff_earliest_date) & (df['Date'] <= cutoff_latest_date)\n",
    "            dataframes[df_name] = df[mask]\n",
    "        \n",
    "        # Access the updated DataFrames\n",
    "        balance_sheet_df = dataframes['balance_sheet_df'].reset_index().drop(columns=[\"Index\", \"Ticker\"])\n",
    "        cashflow_statement_df = dataframes['cashflow_statement_df'].reset_index().drop(columns=[\"Index\", \"Ticker\"])\n",
    "        income_statement_df = dataframes['income_statement_df'].reset_index().drop(columns=[\"Index\", \"Ticker\"])\n",
    "        market_stats_df = dataframes['market_stats_df'].reset_index().drop(columns=[\"Index\", \"Ticker\"])\n",
    "        \n",
    "        # Saves compiled dataframes as csv files under compiled_CSV\n",
    "        os.makedirs('compiled_CSV', exist_ok=True)  \n",
    "        balance_sheet_df.to_csv('compiled_CSV/balance_sheet.csv') \n",
    "        cashflow_statement_df.to_csv('compiled_CSV/cashflow_statement.csv') \n",
    "        income_statement_df.to_csv('compiled_CSV/income_statement.csv') \n",
    "        market_stats_df.to_csv('compiled_CSV/market_stats.csv') \n",
    "    @task()\n",
    "    def load(dataframes: object):\n",
    "        \"\"\"\n",
    "        #### Load task\n",
    "        \"\"\"\n",
    "        access_key = os.environ.get(\"AWS_ACCESS_KEY_ID\")\n",
    "        secret_key = os.environ.get(\"AWS_SECRET_ACCESS_KEY\")\n",
    "        # TODO: Remove this return\n",
    "        return True\n",
    "        def get_file_paths(folder_path):\n",
    "            file_paths = []\n",
    "            for root, dirs, files in os.walk(folder_path):\n",
    "                for file in files:\n",
    "                    if file.endswith('.csv'):  # Filter CSV files\n",
    "                        file_path = os.path.join(root, file)\n",
    "                        file_paths.append(file_path)\n",
    "            return file_paths\n",
    "\n",
    "        def upload_files_to_s3(file_paths, bucket_name):\n",
    "            s3 = boto3.client('s3', aws_access_key_id=access_key, aws_secret_access_key=secret_key)\n",
    "            for file_path in file_paths:\n",
    "                file_name = file_path.split('/')[-1]  # Extract the file name from the path\n",
    "                s3.upload_file(file_path, bucket_name, file_name)\n",
    "                print(f\"Uploaded {file_name} to {bucket_name}\")\n",
    "\n",
    "        # Folder path containing the CSV files\n",
    "        folder_path = os.getcwd() + \"/compiled_CSV\"\n",
    "\n",
    "        # Call the function to get file paths\n",
    "        file_paths = get_file_paths(folder_path)\n",
    "\n",
    "        # Name of the S3 bucket\n",
    "        bucket_name = 'ds4a-c1-team22'\n",
    "\n",
    "        # Call the function to upload files\n",
    "        upload_files_to_s3(file_paths, bucket_name)\n",
    "    \n",
    "    extract()\n",
    "    dataframes = compile()\n",
    "    transform(dataframes)\n",
    "    load(dataframes)\n",
    "tutorial_taskflow_api()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2 (v3.11.2:878ead1ac1, Feb  7 2023, 10:02:41) [Clang 13.0.0 (clang-1300.0.29.30)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
